{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c36002eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T20:02:21.330744Z",
     "iopub.status.busy": "2024-01-27T20:02:21.329881Z",
     "iopub.status.idle": "2024-01-27T20:02:21.334548Z",
     "shell.execute_reply": "2024-01-27T20:02:21.333729Z",
     "shell.execute_reply.started": "2024-01-27T20:02:21.330710Z"
    },
    "papermill": {
     "duration": 0.007254,
     "end_time": "2024-01-27T20:19:25.529086",
     "exception": false,
     "start_time": "2024-01-27T20:19:25.521832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## реализация данной статьи https://habr.com/ru/articles/769124/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc44867",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b86caba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T20:19:25.545209Z",
     "iopub.status.busy": "2024-01-27T20:19:25.544266Z",
     "iopub.status.idle": "2024-01-27T20:19:54.707565Z",
     "shell.execute_reply": "2024-01-27T20:19:54.706144Z"
    },
    "papermill": {
     "duration": 29.174226,
     "end_time": "2024-01-27T20:19:54.710136",
     "exception": false,
     "start_time": "2024-01-27T20:19:25.535910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\r\n",
      "  Obtaining dependency information for langchain from https://files.pythonhosted.org/packages/23/9f/a78357793c96ae5b53b5a31a891ed2fe3b02dc1a11a705dd14da67932c42/langchain-0.1.4-py3-none-any.whl.metadata\r\n",
      "  Downloading langchain-0.1.4-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\r\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.20)\r\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.8.5)\r\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\r\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.3)\r\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\r\n",
      "  Obtaining dependency information for jsonpatch<2.0,>=1.33 from https://files.pythonhosted.org/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting langchain-community<0.1,>=0.0.14 (from langchain)\r\n",
      "  Obtaining dependency information for langchain-community<0.1,>=0.0.14 from https://files.pythonhosted.org/packages/57/00/a798f8124db57eb9e20fe31dc7561e15e9c4607281cddaa4db49f93d7111/langchain_community-0.0.16-py3-none-any.whl.metadata\r\n",
      "  Downloading langchain_community-0.0.16-py3-none-any.whl.metadata (7.8 kB)\r\n",
      "Collecting langchain-core<0.2,>=0.1.16 (from langchain)\r\n",
      "  Obtaining dependency information for langchain-core<0.2,>=0.1.16 from https://files.pythonhosted.org/packages/2f/6f/ecfff9820d55888e8cfc49eb4bf543a5d2eec047148f7a85a3022d956275/langchain_core-0.1.16-py3-none-any.whl.metadata\r\n",
      "  Downloading langchain_core-0.1.16-py3-none-any.whl.metadata (6.0 kB)\r\n",
      "Collecting langsmith<0.1,>=0.0.83 (from langchain)\r\n",
      "  Obtaining dependency information for langsmith<0.1,>=0.0.83 from https://files.pythonhosted.org/packages/a1/3f/0808382e9d0b504e727dbaf86f1fcbe59472cac17a205644a5f78b11c36b/langsmith-0.0.83-py3-none-any.whl.metadata\r\n",
      "  Downloading langsmith-0.0.83-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.24.3)\r\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.10.12)\r\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\r\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\r\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\r\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.0)\r\n",
      "Requirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.16->langchain) (3.7.1)\r\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2,>=0.1.16->langchain)\r\n",
      "  Obtaining dependency information for packaging<24.0,>=23.2 from https://files.pythonhosted.org/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl.metadata\r\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.5.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.11.17)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.3.0)\r\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.1.3)\r\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\r\n",
      "Downloading langchain-0.1.4-py3-none-any.whl (803 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\r\n",
      "Downloading langchain_community-0.0.16-py3-none-any.whl (1.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langchain_core-0.1.16-py3-none-any.whl (230 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.3/230.3 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langsmith-0.0.83-py3-none-any.whl (49 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: packaging, jsonpatch, langsmith, langchain-core, langchain-community, langchain\r\n",
      "  Attempting uninstall: packaging\r\n",
      "    Found existing installation: packaging 21.3\r\n",
      "    Uninstalling packaging-21.3:\r\n",
      "      Successfully uninstalled packaging-21.3\r\n",
      "  Attempting uninstall: jsonpatch\r\n",
      "    Found existing installation: jsonpatch 1.32\r\n",
      "    Uninstalling jsonpatch-1.32:\r\n",
      "      Successfully uninstalled jsonpatch-1.32\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\r\n",
      "cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\r\n",
      "cuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\r\n",
      "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\r\n",
      "jupyterlab 4.0.10 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "jupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\r\n",
      "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\r\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "ydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed jsonpatch-1.33 langchain-0.1.4 langchain-community-0.0.16 langchain-core-0.1.16 langsmith-0.0.83 packaging-23.2\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "082c31e8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-01-27T20:19:54.731386Z",
     "iopub.status.busy": "2024-01-27T20:19:54.731015Z",
     "iopub.status.idle": "2024-01-27T20:19:58.810564Z",
     "shell.execute_reply": "2024-01-27T20:19:58.809145Z"
    },
    "papermill": {
     "duration": 4.092767,
     "end_time": "2024-01-27T20:19:58.812410",
     "exception": true,
     "start_time": "2024-01-27T20:19:54.719643",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'peft'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoPeftModelForCausalLM\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'peft'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "adapt_model_name = \"IlyaGusev/saiga_mistral_7b_lora\"\n",
    "base_model_name = \"Open-Orca/Mistral-7B-OpenOrca\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "              base_model_name,\n",
    "              trust_remote_code=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "device_map = {\"\": 0}\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "              adapt_model_name,\n",
    "              device_map=device_map,\n",
    "              torch_dtype=torch.bfloat16)\n",
    "\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac7472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T18:39:30.286116Z",
     "iopub.status.busy": "2024-01-27T18:39:30.285829Z",
     "iopub.status.idle": "2024-01-27T18:39:32.623709Z",
     "shell.execute_reply": "2024-01-27T18:39:32.622678Z",
     "shell.execute_reply.started": "2024-01-27T18:39:30.286091Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "sent_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "sent_model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4990fc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T18:39:39.413942Z",
     "iopub.status.busy": "2024-01-27T18:39:39.413569Z",
     "iopub.status.idle": "2024-01-27T18:39:39.421702Z",
     "shell.execute_reply": "2024-01-27T18:39:39.420683Z",
     "shell.execute_reply.started": "2024-01-27T18:39:39.413914Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embedding(sentence):\n",
    "    \n",
    "    #Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def _mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    encoded_input = sent_tokenizer([sentence], padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = sent_model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = _mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7f167",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T18:39:44.585225Z",
     "iopub.status.busy": "2024-01-27T18:39:44.584382Z",
     "iopub.status.idle": "2024-01-27T18:39:44.589442Z",
     "shell.execute_reply": "2024-01-27T18:39:44.588469Z",
     "shell.execute_reply.started": "2024-01-27T18:39:44.585194Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "answers = []\n",
    "emb_database = torch.empty((0, 384), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c16adac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:06:37.040056Z",
     "iopub.status.busy": "2024-01-27T19:06:37.039682Z",
     "iopub.status.idle": "2024-01-27T19:06:37.046881Z",
     "shell.execute_reply": "2024-01-27T19:06:37.046018Z",
     "shell.execute_reply.started": "2024-01-27T19:06:37.040030Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/zulip-data/prompts.txt\", 'r') as f:\n",
    "    text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4802f9dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:06:38.920063Z",
     "iopub.status.busy": "2024-01-27T19:06:38.919703Z",
     "iopub.status.idle": "2024-01-27T19:06:38.925035Z",
     "shell.execute_reply": "2024-01-27T19:06:38.924088Z",
     "shell.execute_reply.started": "2024-01-27T19:06:38.920036Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(*text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb442e41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:36:48.699052Z",
     "iopub.status.busy": "2024-01-27T19:36:48.698701Z",
     "iopub.status.idle": "2024-01-27T19:36:49.525756Z",
     "shell.execute_reply": "2024-01-27T19:36:49.524692Z",
     "shell.execute_reply.started": "2024-01-27T19:36:48.699026Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa555aed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:53:56.833297Z",
     "iopub.status.busy": "2024-01-27T19:53:56.832601Z",
     "iopub.status.idle": "2024-01-27T19:53:56.866291Z",
     "shell.execute_reply": "2024-01-27T19:53:56.865389Z",
     "shell.execute_reply.started": "2024-01-27T19:53:56.833269Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_ans = pd.read_excel(\"/kaggle/input/zulip-xlsx/messages.xlsx\", sheet_name=\"wekan_ans\", names=['answer'])\n",
    "text_q = pd.read_excel(\"/kaggle/input/zulip-xlsx/messages.xlsx\", sheet_name=\"wekan_q\", names=['questions'])\n",
    "text_q_ans = pd.concat([text_q, text_ans], axis=1)\n",
    "text_q_ans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c7e05d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:56:09.729329Z",
     "iopub.status.busy": "2024-01-27T19:56:09.728946Z",
     "iopub.status.idle": "2024-01-27T19:56:09.736209Z",
     "shell.execute_reply": "2024-01-27T19:56:09.735314Z",
     "shell.execute_reply.started": "2024-01-27T19:56:09.729287Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_q_ans['q_ans'] = text_q_ans['questions'] + text_q_ans['answer']\n",
    "text_q_ans['q_ans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4936dc9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:56:25.525969Z",
     "iopub.status.busy": "2024-01-27T19:56:25.525251Z",
     "iopub.status.idle": "2024-01-27T19:56:25.530606Z",
     "shell.execute_reply": "2024-01-27T19:56:25.529481Z",
     "shell.execute_reply.started": "2024-01-27T19:56:25.525939Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = lambda x : ' '.join(x)\n",
    "text = f(text_q_ans['q_ans']) + \"{question}\\nbot: Вот ответ на ваш вопрос длиной не более 10 слов\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262e46e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T19:56:30.578778Z",
     "iopub.status.busy": "2024-01-27T19:56:30.577926Z",
     "iopub.status.idle": "2024-01-27T19:56:30.583007Z",
     "shell.execute_reply": "2024-01-27T19:56:30.581824Z",
     "shell.execute_reply.started": "2024-01-27T19:56:30.578745Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "info_prompt_less10 = PromptTemplate.from_template(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b91161",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T20:01:37.035012Z",
     "iopub.status.busy": "2024-01-27T20:01:37.034633Z",
     "iopub.status.idle": "2024-01-27T20:01:37.042081Z",
     "shell.execute_reply": "2024-01-27T20:01:37.041163Z",
     "shell.execute_reply.started": "2024-01-27T20:01:37.034985Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_answer(info_prompt, question):\n",
    "    \n",
    "    prompt = info_prompt.format(question=question)   \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), \n",
    "                            top_p=0.5,\n",
    "                            temperature=0.3,\n",
    "                            attention_mask=inputs[\"attention_mask\"],\n",
    "                            max_new_tokens=50,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            do_sample=True)\n",
    "\n",
    "    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    parsed_answer = output.split(\"Вот ответ на ваш вопрос длиной не более 10 слов:\")[1].strip()\n",
    "\n",
    "    if \"bot:\" in parsed_answer:\n",
    "        parsed_answer = parsed_answer.split(\"bot:\")[0].strip()\n",
    "\n",
    "    return parsed_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99466d11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T20:01:37.590821Z",
     "iopub.status.busy": "2024-01-27T20:01:37.590476Z",
     "iopub.status.idle": "2024-01-27T20:01:37.620600Z",
     "shell.execute_reply": "2024-01-27T20:01:37.619636Z",
     "shell.execute_reply.started": "2024-01-27T20:01:37.590796Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Не зачтены часы в карточке в wekan\" \n",
    "emb = get_embedding(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ba991",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T20:01:37.999594Z",
     "iopub.status.busy": "2024-01-27T20:01:37.998992Z",
     "iopub.status.idle": "2024-01-27T20:01:38.007457Z",
     "shell.execute_reply": "2024-01-27T20:01:38.006404Z",
     "shell.execute_reply.started": "2024-01-27T20:01:37.999560Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cos_sim(question):\n",
    "    cos_sim = F.cosine_similarity(emb_database, emb, dim=1, eps=1e-8)\n",
    "    return cos_sim\n",
    "  \n",
    "get_cos_sim(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9819d26b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T20:01:38.442821Z",
     "iopub.status.busy": "2024-01-27T20:01:38.442454Z",
     "iopub.status.idle": "2024-01-27T20:01:45.647779Z",
     "shell.execute_reply": "2024-01-27T20:01:45.646834Z",
     "shell.execute_reply.started": "2024-01-27T20:01:38.442795Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer = get_answer(info_prompt_less10, question)\n",
    "emb_database = torch.cat((emb_database, emb), 0)\n",
    "answers.append(answer)\n",
    "print(f'Answer from model: {answer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b686cee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T20:16:44.414591Z",
     "iopub.status.busy": "2024-01-27T20:16:44.413749Z",
     "iopub.status.idle": "2024-01-27T20:16:44.419173Z",
     "shell.execute_reply": "2024-01-27T20:16:44.418093Z",
     "shell.execute_reply.started": "2024-01-27T20:16:44.414560Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Не зачтены часы в карточке в wekan\",\n",
    "    \"Не  отображается доска в wekan\",\n",
    "    \"Не зачисляются часы в карточке\",\n",
    "    \"Руководитель не может проставить часы\",\n",
    "    \"Не найдена доска в wekan\",\n",
    "    \"Можно ли удалить неверно зачтенную карточку?\",\n",
    "    \"Не получается авторизоваться в wekan\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3338494",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T20:16:44.807476Z",
     "iopub.status.busy": "2024-01-27T20:16:44.806477Z",
     "iopub.status.idle": "2024-01-27T20:16:52.118188Z",
     "shell.execute_reply": "2024-01-27T20:16:52.117254Z",
     "shell.execute_reply.started": "2024-01-27T20:16:44.807438Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for q in questions:\n",
    "    print(q)\n",
    "    emb = get_embedding(q)\n",
    "#     answer = get_answer(info_prompt_less10, q)\n",
    "#     emb_database = torch.cat((emb_database, emb), 0)\n",
    "#     answers.append(answer)\n",
    "#     print(f'Answer from model: {answer}')\n",
    "    \n",
    "    cos_sim = get_cos_sim(q)\n",
    "    max_value, max_index = torch.max(get_cos_sim(q), dim=0)\n",
    "\n",
    "    if max_value > 0.9:\n",
    "        answer = answers[max_index]\n",
    "        print(f'DATABASE: {answer}')\n",
    "    else:\n",
    "        answer = get_answer(info_prompt_less10, q)\n",
    "        emb_database = torch.cat((emb_database, emb), 0)\n",
    "        answers.append(answer)\n",
    "        print(f'MODEL: {answer}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40443dd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4363614,
     "sourceId": 7494135,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4363703,
     "isSourceIdPinned": false,
     "sourceId": 7494253,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30636,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 38.633542,
   "end_time": "2024-01-27T20:20:00.145538",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-27T20:19:21.511996",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
